{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Concepts of Data Mining and Machine Learning \n",
    "* Author: Johannes Maucher\n",
    "* Last Update: 25.10.2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Data Mining Process\n",
    "The **Cross-industry standard process for data mining (CRISP)** proposes a common approach for realizing data mining projects: \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/CRISPsmall.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "In the first phase of CRISP the overall business-case, which shall be supported by the data mining process must be clearly defined and understood. Then the goal of the data mining project itself must be defined. This includes the specification of metrics for measuring the performance of the data mining project. \n",
    "\n",
    "In the second phase data must be gathered, accessed, understood and described. Quantitiy and qualitity of the data must be assessed on a high-level. \n",
    "\n",
    "In the third phase data must be investigated and understood more thoroughly. Common means for understanding data are e.g. visualization and the calculation of simple statistics. Outliers must be detected and processed, sampling rates must be determined, features must be selected and eventually be transformed to other formats.  \n",
    "\n",
    "In the modelling phase various algorithms and their hyperparameters are selected and applied. Their performance on the given data is determined in the evaluation phase. \n",
    "\n",
    "The output of the evaluation is usually fed back to the first phases (business- and data-understanding). Applying this feedback the techniques in the overall process are adapted and optimized. Usually only after several iterations of this process the evaluation yields good results and the project can be deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Definition, Concepts, Categories\n",
    "\n",
    "Machine Learning constitutes one of the 4 categories of Artificial Intelligence (AI). As shown in the image below, the other categories are *Search and Planning*, *Knowledge and Inference* and *Modelling of Uncertainty*.  \n",
    "\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/categoriesEnglish.png\" alt=\"Drawing\" width=\"600\">\n",
    "\n",
    "Currently, Machine Learning is by far the most important AI category.\n",
    "\n",
    "The following cartoon depicts the idea of supervised learning. A teacher provides labels for the input and a relation (model) between input and label is learned. Actually, the cartoon sketches a crucial problem of supervised Machine Learning: If we have only a small amount of training data, the learned model is overfits to a probably irrelevant feature.\n",
    "\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/MLcartoon.JPG\" alt=\"Drawing\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "### Definition Machine Learning\n",
    "There is no unique definition of Machine Learning. One of the most famous definitions has been formulated in [Tom Mitchell, Machine Learning](http://www.cs.cmu.edu/~tom/mlbook.html):\n",
    "\n",
    "\n",
    "* A computer is said to learn from **experience E** with respect to some **task T** and some **performance measure P** , if its performance on T, as measured by P, improves with experience E.\n",
    "\n",
    "\n",
    "This definition has a very pragmatic implication: At the very beginning of any Machine Learning project one should specify T, E and P! In some projects the determination of these elements is trivial, in particular the *task T* is usually clear. However, the determination of *experience E* and *performance measure P* can be sophisticated. Spend time to specify these elements. It will help you to understand, design and evaluate your project. \n",
    "\n",
    "**Examples:** What would be T, E and P for\n",
    "* a spam-classifier\n",
    "* an intelligent search-engine, which provides individual results on queries\n",
    "* a recommender-system for an online-shop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structure\n",
    "\n",
    "Each ML algorithm requires numeric vectors of unique length at it's input. Each vector represents an **instance**, which itself is described by a set of $K$ **features**. Usually we have many such vectors for training and testing and by stacking these vectors together we end up at the following two-dimensional data array. This is the standard data-structure for ML.\n",
    "\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/dataMatrix.png\" style=\"width:500px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be challenging to transform the given data into this format. Some examples are given below. \n",
    "\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/mlDataExamples.png\" style=\"width:700px\" align=\"center\">\n",
    "\n",
    "* In object recognition the instances are images and the features are all pixel-values of the image. An image of size $r \\times c$ with $z$ channels is then described by $K=r \\cdot c \\cdot z$ features.\n",
    "* In document classification a common form of representation (the *Bag-of-Word (BoW)* model) is to describe each document (row in the matrix) by the words it contains, i.e. the columns of the 2-dimensional data-structure are the words of the entire vocabulary and the entries in this 2-dimensional array indicate how often a word appears in the corresponding document\n",
    "* for a recommender-system of the online-shop the instances are the customers and each customer is described by the products he or she already purchased. \n",
    "* In predictive maintenance the input is usually a set of sensor values (tracked over time) and the output is the time-difference between the sensor-measurement and the day of maintenance.\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories\n",
    "\n",
    "The field of Machine Learning is usually categorized with respect to two dimensions: The first dimension is the question *What shall be learned?* and the second asks for *How shall be learned?*. The resulting 2-dimensional matrix is depicted below:\n",
    "\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/mlCategories.png\" style=\"width:800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an abstract level there exist 4 answers on the first question. One can either learn \n",
    "\n",
    "* a **classifier**, e.g. object recognition, spam-filter, Intrusion detection, ...\n",
    "* a **regression-model**, e.g. time-series prediction, like weather- or stock-price forecasts, range-prediction for electric vehicles, estimation of product-quantities, ...\n",
    "* **associations between instances**, e.g. document clustering, customer-grouping, quantisation problems, automatic playlist-generation, ....\n",
    "* **associations between features**, e.g. market basket analysis (customer who buy cheese, also buy wine, ...)\n",
    "* **policy**, e.g. for automatic driving or games \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/classReg.PNG\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "On the 2nd dimension, which asks for *How to learn?*, the answers are:\n",
    "\n",
    "* **supervised:** This category requires a *teacher* who provides labels (target-values) for each training-element. For example in face-recognition the teacher most label the inputs (pictures of faces) with the name of the corresponding persons. In general labeling is expensive and labeled data is scarce. \n",
    "* **unsupervised learning:** In this case training data consists only of inputs - no teacher is required for labeling. For example pictures can be clustered, such that similar pictures are assigned to the same group.\n",
    "* **Reinforcement learning:** In this type no teacher who lables each input-instance is available. However, there is a critics-element, which provides feedback from time-to-time. For example an intelligent agent in a computer game maps each input state to a corresponding action. Only after a possibly long sequence of actions the agent gets feedback in form of an increasing/decreasing High-Score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearning.png\" style=\"width:800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Learned Modell:**\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningApply.png\" style=\"width:800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningUnsupervised.png\" style=\"width:800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply learned Model:**\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningUnsupervisedApply.png\" style=\"width:800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Associations between Instances (Clustering) and Associations between Features (Association Rule Mining):**\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/unsupervisedML.png\" style=\"width:500px\" align=\"center\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demo of k-means Clustering:**\n",
    "\n",
    "[K-Means Cluster Demo](https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning: Learning from Feedback\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/reinforcementlearningExamples.png\" style=\"width:800px\" align=\"center\">\n",
    "\n",
    "---\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy. A policy defines for each state $S$ of the environment an action $A$, which shall be executed in this state. The optimal policy is the policy for which the expected cumulative future reward is maximal. \n",
    "\n",
    "Reinforcement-Learning is Trial-and-Error learning. The agent selects an action $A_t$ in it's current state $S_t$. After the execution of this action the environment state changes, the new state is $S_{t+1}$. Moreover, the agent may receive a positive or negative reward $R_{t+1}$ for his previous action. These received rewards are applied to adapt the future action-selection. Since the reward is only available after performing actions, this type of learning is also called **learning with a critic** - in contrast to learning with a teacher (i.e. supervised learning).\n",
    "\n",
    "Reinforcement-Learning works in non-deterministic environments (i.e. for a given state-action pair the successive state is not known for sure). Reinforcement-Learning can also be applied, if the environment is totally unknown to the agent (i.e. the agent doesn't know the set of possible successive states and the set of possible rewards).\n",
    "\n",
    "During the iterative training process in unknown environments the agent must explore. The challenge is to find a good **explore-exploit** trade off. Explore means going along new, not yet visited paths. Exploit means applying for the best action learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorization on Application-Level\n",
    "<img src=\"http://maucher.home.hdm-stuttgart.de/Pics/MLapplicationCategories.png\" style=\"width:800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Scheme for Machine Learning\n",
    "\n",
    "In Machine Learning one distinguishes  \n",
    "* training-phase,\n",
    "* validation-phase,\n",
    "* test-phase, \n",
    "* operational phase.\n",
    "\n",
    "Training and validation are shown in the image below. \n",
    "\n",
    "1. In the **training phase** training-data is applied to learn a general model. The model either describes the structure of the training data (in the case of unsupervised learning) or a function, which maps input-data to outputs.\n",
    "\n",
    "2. **Validation phase:** During the training-phase the model is fitted to the training-data. However, the primal goal is not to find a model, which is maximally fitted to training data. Instead we like to learn a model, which **generalizes well**, i.e. it shall be good on new data, which has not been seen during training. For this a validation-data partition, which is disjoint to the training-data partition, is applied to the model. This means, that for each input of the validation-partition the learned models prediction is calculated and compared to the true output (label). In this way an error-statistic, more general: a performance-measure, can be calculated. The performance measure on the validation data is used to assess the learned model, to compare different models and to select the best trained model.\n",
    "\n",
    "3. **Test phase:** In order to estimate the selected models performance in real life, one applies a test-data partition, which is disjoint to training-data and validation-data. The performance on the test-data tells us the accuracy we can expect in the operational mode.\n",
    "\n",
    "4. **Operational mode:** If the performance in the preceding test phase is sufficient for our purpose, the model can be deployed to the target device. In the operational mode, the model receives new input data and has to calculate the corresponding output (class, prediction, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/Learning.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross-Validation is the standard validation method if labeled data is rare. The entire set of labeled data is partitioned into k ($k=10$ in the example below) disjoint subsets. The entire evaluation consists of k iterations. In the i.th iteration, the i.th partition (subset) is applied for validation, all other partitions are applied for training the model. In each iteration the model's performance, e.g. accuracy, is determined on the validation-partition. Finally, the overall performance is the average performance over all k performance values.  \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/CrossValidation.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics\n",
    "\n",
    "Below $r_i$ denotes the true label, given in the labeled dataset and $y_i$ denotes the corresponding label as predicted by the learned model. Obviously the goal is to learn a model, whose predictions $y_i$ are as close as possible to the true labels $r_i$. For classification and regression there exists different metrics to measure the difference between true and predicted labels. The most important metrics are summarized below. Metrics, provided by scikit-learn, can be found here: [Scores in scikit-learn](http://scikit-learn.org/stable/modules/model_evaluation.html). \n",
    "\n",
    "#### Classification\n",
    "\n",
    "For classification the most prominent metric is **accuracy**, which is just the rate of correct classifications. For the analysis of a classifier, determination of accuracy alone is not sufficient. The metrics defined below provide more subtle information on correct and erroneous events. All of the defined evaluation metrics can be obtained from the confusion matrix. For a binary classifier, the **confusion matrix** is depicted below. For a *K*-class classifier, the confusion matrix has size $K \\times K$. The rows correspond to the true labels, the columns to the predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/confusionMat.png\" style=\"width:300px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** The rate of overall correct classifications: \n",
    "\n",
    "$$\n",
    "ACC=\\frac{TP+TN}{FP+FN+TP+TN}\n",
    "$$\n",
    "\n",
    "**Error Rate:** The rate of overall erroneous classifications: \n",
    "\n",
    "$$\n",
    "ERR=\\frac{FP+FN}{FP+FN+TP+TN}\n",
    "$$\n",
    "\n",
    "**False Positive Rate:** \n",
    "\n",
    "$$\n",
    "FPR=\\frac{FP}{FP+TN}\n",
    "$$\n",
    "\n",
    "**True Positive Rate:** \n",
    "\n",
    "$$\n",
    "TPR=\\frac{TP}{FN+TP}\n",
    "$$\n",
    "\n",
    "**Precision:** How much of the samples, which have been classified as *positive* are actual *positive* \n",
    "\n",
    "$$\n",
    "PRE=\\frac{TP}{FP+TP}\n",
    "$$ \n",
    "\n",
    "**Recall:**(=TPR): How much of the true *positive* samples has been classified as *positive* \n",
    "\n",
    "$$\n",
    "REC=\\frac{TP}{FN+TP}\n",
    "$$\n",
    "\n",
    "**F1-Score:** Harmonic mean of Precision and Recall \n",
    "\n",
    "$$\n",
    "F1=2\\frac{PRE \\cdot REC }{PRE + REC}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "Also regression models can be scored by a variety of metrics. The most prominent are \n",
    "\n",
    "* mean absolute error (MAE)\n",
    "* mean squared error (MSE)\n",
    "* median absolute error (MEDE)\n",
    "* coefficient of determination ($R^2$) \n",
    "\n",
    "If $y_i$ is the predicted value for the i.th element and $r_i$ is it's true value, then these metrics are defined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{array}[lcl]\n",
    " NMAE & = &   \\frac{1}{N}\\sum\\limits_{i=1}^N |y_i-r_i| \\\\\n",
    " MSE & = &   \\frac{1}{N}\\sum\\limits_{i=1}^N (y_i-r_i)^2  \\\\\n",
    " MEDE & = &  median\\left( \\; |y_i-r_i|, \\; \\forall \\; i \\; \\in [1,..,N]\\right) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2  =  1- \\frac{SS_e}{SS_r}, \\quad \\mbox{ with } SS_e=\\sum_{i=1}^N(r_i-y_i)^2, \\quad  SS_r=\\sum_{i=1}^N(r_i-\\overline{r})^2 \\quad \\mbox { and } \\quad \\overline{r}=\\frac{1}{N} \\sum_{i=1}^N r_i\n",
    "$$\n",
    "\n",
    "Another frequently used regression metric is the **Root Mean Squared Logarithmic Error (RMSLE)**, which is caluclated as follows:\n",
    "\n",
    "$$\n",
    "RMSLE = \\sqrt{\\frac{1}{N} \\sum\\limits_{i=1}^N(\\ln(r_i)-\\ln(y_i))^2}\n",
    "$$\n",
    "\n",
    "The RMSLE is well suited for the case that the error (i.e. the difference between $y_i$ and $r_i$) increases with the values of $r_i$. Then large errors at high values of $r_i$ are weighted less by RMSLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting / Underfitting\n",
    "\n",
    "The goal of supervised machine learning is to find a model, which performs well on new data, i.e. data, which has not be seen during training. It is in general no problem, to find an algorithm, which learns a model, that is well adapted to the given training data. However, it is a big challenge to find a model, that performs well on previously unseen data. \n",
    "\n",
    "**Overfitting** means, that a model has been strongly adapted to the training data, but it performs bad on new data. Algorithms of low-bias are able to learn models, which are strongly fittet to training data. However, then often the variance and the probability of overfitting are high.  \n",
    "\n",
    "**Underfitting** means, that the learned model is weakly adapted to the training data. Algorithms of high-bias yield an increased probability of underfitting. \n",
    "\n",
    "The image below sketches the relation between bias, variance, over- and underfitting (Image source: [https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229))\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/biasvarianceoverfitting.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
