{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representations for Words and Texts\n",
    "* Author: Johannes Maucher\n",
    "* Last Update: 25.10.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In previous sections, e.g. [02RegressionPipe.ipynb](02RegressionPipe.ipynb) different types of data, numeric and categorial, have been applied. It has been shown how categorical data is mapped to numeric values or numeric vectors, such that it can be applied as input of a Machine Learning algorithm.\n",
    "\n",
    "Another type of data is text, either single words, sentences, sections or entire documents. How to map these types to numeric representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-Hot-Encoding of Single Words\n",
    "A very simple option for representing single words as numeric vectors is One-Hot-Encoding. This type of encoding has already been introduced above for modelling non-binary categorial features. Each possible value (word) is uniquely mapped to an index, and the associated vector contains only zeros, except at the position of the value's (word's) index.\n",
    "\n",
    "For example, assume that the entire set of possible words is \n",
    "\n",
    "$$\n",
    "V=(\\mbox{all, and, at, boys, girls, home, kids, not, stay}).\n",
    "$$\n",
    "\n",
    "Then a possible One-Hot-Encoding of these words is then\n",
    "\n",
    "|       |   |   |   |   |   |   |   |   |   |\n",
    "|-------|---|---|---|---|---|---|---|---|---|\n",
    "| all   | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| and   | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| at    | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| boys  | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "| girls | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
    "| home  | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
    "| kids  | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n",
    "| not   | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n",
    "| stay  | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True,precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **word-index** is just a one-to-one mapping of words to integers. Usually the word-index defines the One-Hot-Encoding of words: If $i(w)$ is the index of word $w$, then the One-Hot-Encoding of $v(w)$ is a vector, which consists of only zeros, except at the element at position $i(w)$. The value at this position is 1.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>girls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0    all\n",
       "1    and\n",
       "2     at\n",
       "3   boys\n",
       "4  girls\n",
       "5   home\n",
       "6   kids\n",
       "7    not\n",
       "8   stay"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simpleWordDF=pd.DataFrame(data=[\"all\", \"and\", \"at\", \"boys\", \"girls\", \"home\", \"kids\", \"not\", \"stay\"])\n",
    "print(\"\\nWord Index:\")\n",
    "simpleWordDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corresponding One-Hot-Encoding\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_all</th>\n",
       "      <th>_and</th>\n",
       "      <th>_at</th>\n",
       "      <th>_boys</th>\n",
       "      <th>_girls</th>\n",
       "      <th>_home</th>\n",
       "      <th>_kids</th>\n",
       "      <th>_not</th>\n",
       "      <th>_stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _all   _and    _at  _boys  _girls  _home  _kids   _not  _stay\n",
       "0   True  False  False  False   False  False  False  False  False\n",
       "1  False   True  False  False   False  False  False  False  False\n",
       "2  False  False   True  False   False  False  False  False  False\n",
       "3  False  False  False   True   False  False  False  False  False\n",
       "4  False  False  False  False    True  False  False  False  False\n",
       "5  False  False  False  False   False   True  False  False  False\n",
       "6  False  False  False  False   False  False   True  False  False\n",
       "7  False  False  False  False   False  False  False   True  False\n",
       "8  False  False  False  False   False  False  False  False   True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nCorresponding One-Hot-Encoding\")\n",
    "pd.get_dummies(simpleWordDF,prefix=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "One-Hot-Encoding of words suffer from crucial drawbacks: \n",
    "\n",
    "1. The vectors are usually very long - there length is given by the number of words in the vocabulary. Moreover, the vectors are quite sparse, since the set of words appearing in one document is usually only a very small part of the set of all words in the vocabulary.\n",
    "2. Semantic relations between words are not modelled. This means that in this model there is no information about the fact that word *car* is more related to word *vehicle* than to word *lake*. \n",
    "3. In the BoW-model of documents word order is totally ignored. E.g. the model can not distinguish if word *not* appeared immediately before word *good* or before word *bad*.  \n",
    "\n",
    "All of these drawbacks can be solved by applying *Word Embeddings* and by the way the resulting *Word Empeddings* are passed e.g. to the input of Recurrent Neural Networks, Convolutional Neural Networks or Transformers (see later chapters of this lecture). \n",
    "\n",
    "Word embeddings have revolutionalized many fields of Natural Language Processing since their efficient neural-network-based generation has been published in [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) by Mikolov et al (2013). Word embeddings map words into vector-spaces such that semantically or syntactically related words are close together, whereas unrelated words are far from each other. Moreover, it has been shown that the word-embeddings, generated by *word2vec*-techniques *CBOW* or *Skipgram*, are well-structured in the sense that also relations such as *is-capital-of*, *is-female-of*, *is-plural-of* are encoded in the vector space. In this way questions like *woman is to queen, as man is to ?* can be answered by simple operations of linear algebra in the word-vector-space. Compared to the length of one-hot encoded word-vectors, word-embedding-vectors are short (typical lengths in the range from 100-300) and dense (float-values). \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/dsm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "*CBOW* and *Skipgram*, are techniques to learn word-embeddings, i.e. a mapping of words to vectors by relatively simple neural networks. Usually large corpora are applied for learning, e.g. the entire Wikipedia corpus in a given language. Today, pretrained models for the most common languages are available, for example from [FastText project](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Word Modell of documents\n",
    "\n",
    "### Term Frequencies\n",
    "\n",
    "The conventional model for representing texts of arbitrary length as numeric vectors, is the **Bag-of-Words** model. \n",
    "In this model each word of the underlying vocabulary corresponds to one column and each document (text) corresponds to a single row of a matrix. The entry in row $i$, column $j$ is just the term-frequency $tf_{i,j}$ of word $j$ in document $i$. \n",
    "\n",
    "For example, assume, that we have only two documents\n",
    "\n",
    "* Document 1: *not all kids stay at home*\n",
    "* Document 2: *all boys and girls stay not at home*\n",
    "\n",
    "The BoW model of these documents is then\n",
    "\n",
    "|            | all | and | at   | boys | girls | home | kids | not  | stay |\n",
    "|------------|-----|-----|------|------|-------|------|------|------|------|\n",
    "| Document 1 | 1   | 0   | 1    | 0    | 0     | 1    | 1    | 1    | 1    |\n",
    "| Document 2 | 1   | 1   | 1    | 1    | 1     | 1    | 0    | 1    | 1    |\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['all', 'and', 'at', 'boys', 'girls', 'home', 'kids', 'not', 'stay'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['not all kids stay at home.',\n",
    "          'all boys and girls stay not at home.',\n",
    "         ]\n",
    "BoW = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency Inverse Document Frequency\n",
    "\n",
    "Instead of the term-frequency $tf_{i,j}$ it is also possible to fill the BoW-vector with \n",
    "* a binary indicator which indicates if the term $j$ appears in document $i$\n",
    "* the tf-idf-values \n",
    "\n",
    "$$\n",
    "tfidf_{i,j}=tf_{i,j} \\cdot log \\frac{N}{df_j},\n",
    "$$ \n",
    "\n",
    "where $df_j$ is the frequency of documents, in which term $j$ appears, and $N$ is the total number of documents. The advantage of tf-idf, compared to just tf-entries, is that in *tf-idf* the term-frequency *tf* is multiplied by a value *idf*,  which is small for less informative words, i.e. words which appear in many documents, and high for words, which appear in only few documents. It is assumed, that words, which appear only in a few documents have a stronger *semantic focus* and are therefore more important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_BoW = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.379, 0.   , 0.379, 0.   , 0.   , 0.379, 0.532, 0.379, 0.379],\n",
       "       [0.303, 0.425, 0.303, 0.425, 0.425, 0.303, 0.   , 0.303, 0.303]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_BoW.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By inspecting the output of the previous cell, you may realize that this output doesn't fit to the expectation. We expect the tf-idf value of words which appear in all documents are 0. However, in the example above such words have non-zero tf-idf-values. The reason for this is that **in scikit-learn** the tf-idf is implemented in a slightly different way. As described in [scikit-learn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) scikit-learn calculates the tf-idf as follows:\n",
    "\n",
    "$$\n",
    "tfidf_{i,j}=tf_{i,j} \\cdot ( log \\frac{N}{df_j} + 1),\n",
    "$$ \n",
    "\n",
    "Moreover, from the output of the previous cell we see that the tf-idf-values are normalized such that each vector has an L2-norm of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to generate Wordembeddings? CBOW and Skipgram\n",
    "In 2013 Mikolov et al. published [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf). They proposed quite simple neural network architectures to efficiently create word-embeddings: CBOW and Skipgram. These architectures are better known as **Word2Vec**. In both techniques neural networks are trained for a pseudo-task. After training, the network itself is usually not of interest. However, the learned weights in the input-layer constitute the word-embeddings, which can then be applied for a large field of NLP-tasks, e.g. document classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continous Bag-Of-Words (CBOW)\n",
    "The idea of CBOW is to predict the target word $w_i$, given the $N$ context-words $w_{i-N/2},\\ldots, w_{i-1}, \\quad w_{i+1}, w_{i+N/2}$. \n",
    "In order to learn such a predictor a large but unlabeled corpus is required. The extraction of training-samples from a corpus is sketched in the picture below:\n",
    "\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/CBowTrainSamples.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "In this example a context length of $N=4$ has been applied. The first training-element consists of \n",
    "* the $N=4$ input-words *(happy,families,all,alike)*\n",
    "* the target word *are*.\n",
    "\n",
    "In order to obtain the second training-sample the window of length $N+1$ is just shifted by one to the right. The concrete architecture for CBOW is shown in the picture below. At the input the $N$ context words are one-hot-encoded. The fully-connected *Projection-layer* maps the context words to a vector representation of the context. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary $V$. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears as target for the current context-words at the input.  \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/cbowGramArchitecture.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "After training the CBOW-network the vector representation of word $w$ are the weights from the one-hot encoded word $w$ at the input of the network to the neurons in the projection-layer. I.e. the number of neurons in the projection layer define the length of the word-embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skip-Gram\n",
    "Skip-Gram is similar to CBOW, but has a reversed prediction process: For a given target word at the input, the Skip-Gram model predicts words, which are likely in the context of this target word. Again, the context is defined by the $N$ neighbouring words. The extraction of training-samples from a corpus is sketched in the picture below:\n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/skipGramTrainSamples.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Again a context length of $N=4$ has been applied. The first training-element consists of \n",
    "* the first target word *(happy)* as input to the network \n",
    "* the first context word *(families)* as network-output.\n",
    "\n",
    "The concrete architecture for Skip-gram is shown in the picture below. At the input the target-word is one-hot-encoded. The fully-connected *Projection-layer* outputs the current vector representation of the target-word. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary $V$. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears in the context of the current target-word at the input. \n",
    "\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/skipGramArchitecture.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Word-Embeddings\n",
    "CBOW- and Skip-Gram are possibly the most popular word-embeddings. However, there are more count-based and prediction-based methods to generate them, e.g. Random-Indexing, [Glove](https://nlp.stanford.edu/projects/glove/), [FastText](https://fasttext.cc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to Access Pretrained Word-Embeddings?\n",
    "### Fasttext Word-Embeddings\n",
    "After downloading word embeddings from [FastText](https://fasttext.cc/) they can be imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 2000000\n",
      "Dimension of a word vector: 300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Creating the model\n",
    "#en_model = KeyedVectors.load_word2vec_format('/Users/maucher/DataSets/Gensim/FastText/Gensim/FastText/wiki-news-300d-1M.vec')\n",
    "#en_model = KeyedVectors.load_word2vec_format(r'C:\\Users\\maucher\\DataSets\\Gensim\\Data\\Fasttext\\wiki-news-300d-1M.vec\\wiki-news-300d-1M.vec') #path on surface\n",
    "en_model = KeyedVectors.load_word2vec_format('/Users/johannes/DataSets/Gensim/FastText/fasttextEnglish300.vec')\n",
    "# Getting the tokens \n",
    "words = []\n",
    "for word in en_model.key_to_index:\n",
    "    words.append(word)\n",
    "\n",
    "# Printing out number of tokens available\n",
    "print(\"Number of Tokens: {}\".format(len(words)))\n",
    "\n",
    "# Printing out the dimension of a word vector \n",
    "print(\"Dimension of a word vector: {}\".format(\n",
    "    len(en_model[words[0]])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the words of the first 10 index-positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ,\n",
      "1 the\n",
      "2 .\n",
      "3 and\n",
      "4 to\n",
      "5 of\n",
      "6 a\n",
      "7 </s>\n",
      "8 in\n",
      "9 is\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i,en_model.index_to_key[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vector of word at index 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.052,  0.074, -0.013,  0.045, -0.034,  0.021,  0.007, -0.016,\n",
       "       -0.018, -0.002, -0.102,  0.006,  0.026, -0.003, -0.059, -0.038,\n",
       "        0.016,  0.015, -0.009, -0.018, -0.009, -0.008, -0.018,  0.009,\n",
       "        0.001, -0.094,  0.014,  0.015, -0.039, -0.029,  0.009, -0.025,\n",
       "       -0.01 , -0.221, -0.023, -0.009, -0.032,  0.082,  0.002,  0.028,\n",
       "        0.007, -0.009, -0.035, -0.018, -0.071,  0.063, -0.009, -0.022,\n",
       "       -0.006,  0.052, -0.031,  0.044, -0.011, -0.056,  0.009, -0.067,\n",
       "        0.01 ,  0.057,  0.01 , -0.028,  0.047,  0.005,  0.003,  0.001,\n",
       "        0.044,  0.007, -0.033,  0.009, -0.008,  0.007,  0.092,  0.031,\n",
       "        0.054,  0.028, -0.02 , -0.033,  0.005,  0.036,  0.225,  0.093,\n",
       "       -0.012,  0.009, -0.06 ,  0.068,  0.04 ,  0.001,  0.046, -0.044,\n",
       "        0.006,  0.092, -0.041, -0.015, -0.023,  0.009,  0.059,  0.028,\n",
       "        0.065, -0.057, -0.013,  0.047,  0.035, -0.012, -0.008, -0.131,\n",
       "        0.013, -0.051,  0.011,  0.012, -0.022,  0.039,  0.022,  0.024,\n",
       "        0.004,  0.115,  0.023, -0.047, -0.046, -0.019,  0.008, -0.03 ,\n",
       "       -0.035, -0.029, -0.04 ,  0.024, -0.01 ,  0.058, -0.039, -0.012,\n",
       "       -0.03 ,  0.247, -0.011,  0.036,  0.005,  0.209, -0.102,  0.034,\n",
       "        0.069, -0.071,  0.027, -0.042,  0.008, -0.027,  0.007,  0.004,\n",
       "        0.035, -0.006, -0.446,  0.01 , -0.012, -0.045, -0.17 ,  0.05 ,\n",
       "        0.093, -0.004, -0.004,  0.032,  0.203,  0.061, -0.03 ,  0.023,\n",
       "       -0.019,  0.017,  0.148, -0.018, -0.013,  0.069,  0.033, -0.03 ,\n",
       "        0.043,  0.005,  0.023,  0.01 ,  0.073,  0.008, -0.005,  0.054,\n",
       "       -0.032,  0.051,  0.029, -0.059, -0.   ,  0.049,  0.017, -0.014,\n",
       "        0.036,  0.054, -0.001, -0.059,  0.016, -0.022, -0.02 ,  0.023,\n",
       "       -0.068,  0.018,  0.003,  0.011,  0.047, -0.044,  0.032,  0.02 ,\n",
       "       -0.065,  0.339,  0.07 , -0.022, -0.024, -0.003, -0.003, -0.062,\n",
       "        0.012,  0.038, -0.02 ,  0.024, -0.088,  0.02 , -0.006, -0.026,\n",
       "       -0.019, -0.026,  0.019, -0.042,  0.025,  0.083, -0.01 ,  0.129,\n",
       "        0.062,  0.054,  0.019,  0.042,  0.18 , -0.001, -0.033, -0.056,\n",
       "       -0.016,  0.049,  0.035, -0.042,  0.016, -0.077, -0.066,  0.05 ,\n",
       "        0.01 ,  0.147, -0.071, -0.147,  0.474, -0.017, -0.005,  0.016,\n",
       "        0.055, -0.063, -0.021,  0.012,  0.027,  0.006,  0.066,  0.011,\n",
       "       -0.071, -0.021, -0.078, -0.029, -0.028, -0.157, -0.039,  0.005,\n",
       "        0.02 , -0.003,  0.044,  0.028, -0.039,  0.037, -0.004, -0.016,\n",
       "       -0.073, -0.164,  0.065, -0.006, -0.065, -0.198, -0.041, -0.153,\n",
       "        0.002,  0.013, -0.236, -0.053, -0.004, -0.045,  0.011, -0.033,\n",
       "       -0.055,  0.001,  0.017, -0.044, -0.058,  0.022, -0.078, -0.043,\n",
       "       -0.025,  0.237,  0.   , -0.004], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model[words[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vector of word *the*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.052,  0.074, -0.013,  0.045, -0.034,  0.021,  0.007, -0.016,\n",
       "       -0.018, -0.002, -0.102,  0.006,  0.026, -0.003, -0.059, -0.038,\n",
       "        0.016,  0.015, -0.009, -0.018, -0.009, -0.008, -0.018,  0.009,\n",
       "        0.001, -0.094,  0.014,  0.015, -0.039, -0.029,  0.009, -0.025,\n",
       "       -0.01 , -0.221, -0.023, -0.009, -0.032,  0.082,  0.002,  0.028,\n",
       "        0.007, -0.009, -0.035, -0.018, -0.071,  0.063, -0.009, -0.022,\n",
       "       -0.006,  0.052, -0.031,  0.044, -0.011, -0.056,  0.009, -0.067,\n",
       "        0.01 ,  0.057,  0.01 , -0.028,  0.047,  0.005,  0.003,  0.001,\n",
       "        0.044,  0.007, -0.033,  0.009, -0.008,  0.007,  0.092,  0.031,\n",
       "        0.054,  0.028, -0.02 , -0.033,  0.005,  0.036,  0.225,  0.093,\n",
       "       -0.012,  0.009, -0.06 ,  0.068,  0.04 ,  0.001,  0.046, -0.044,\n",
       "        0.006,  0.092, -0.041, -0.015, -0.023,  0.009,  0.059,  0.028,\n",
       "        0.065, -0.057, -0.013,  0.047,  0.035, -0.012, -0.008, -0.131,\n",
       "        0.013, -0.051,  0.011,  0.012, -0.022,  0.039,  0.022,  0.024,\n",
       "        0.004,  0.115,  0.023, -0.047, -0.046, -0.019,  0.008, -0.03 ,\n",
       "       -0.035, -0.029, -0.04 ,  0.024, -0.01 ,  0.058, -0.039, -0.012,\n",
       "       -0.03 ,  0.247, -0.011,  0.036,  0.005,  0.209, -0.102,  0.034,\n",
       "        0.069, -0.071,  0.027, -0.042,  0.008, -0.027,  0.007,  0.004,\n",
       "        0.035, -0.006, -0.446,  0.01 , -0.012, -0.045, -0.17 ,  0.05 ,\n",
       "        0.093, -0.004, -0.004,  0.032,  0.203,  0.061, -0.03 ,  0.023,\n",
       "       -0.019,  0.017,  0.148, -0.018, -0.013,  0.069,  0.033, -0.03 ,\n",
       "        0.043,  0.005,  0.023,  0.01 ,  0.073,  0.008, -0.005,  0.054,\n",
       "       -0.032,  0.051,  0.029, -0.059, -0.   ,  0.049,  0.017, -0.014,\n",
       "        0.036,  0.054, -0.001, -0.059,  0.016, -0.022, -0.02 ,  0.023,\n",
       "       -0.068,  0.018,  0.003,  0.011,  0.047, -0.044,  0.032,  0.02 ,\n",
       "       -0.065,  0.339,  0.07 , -0.022, -0.024, -0.003, -0.003, -0.062,\n",
       "        0.012,  0.038, -0.02 ,  0.024, -0.088,  0.02 , -0.006, -0.026,\n",
       "       -0.019, -0.026,  0.019, -0.042,  0.025,  0.083, -0.01 ,  0.129,\n",
       "        0.062,  0.054,  0.019,  0.042,  0.18 , -0.001, -0.033, -0.056,\n",
       "       -0.016,  0.049,  0.035, -0.042,  0.016, -0.077, -0.066,  0.05 ,\n",
       "        0.01 ,  0.147, -0.071, -0.147,  0.474, -0.017, -0.005,  0.016,\n",
       "        0.055, -0.063, -0.021,  0.012,  0.027,  0.006,  0.066,  0.011,\n",
       "       -0.071, -0.021, -0.078, -0.029, -0.028, -0.157, -0.039,  0.005,\n",
       "        0.02 , -0.003,  0.044,  0.028, -0.039,  0.037, -0.004, -0.016,\n",
       "       -0.073, -0.164,  0.065, -0.006, -0.065, -0.198, -0.041, -0.153,\n",
       "        0.002,  0.013, -0.236, -0.053, -0.004, -0.045,  0.011, -0.033,\n",
       "       -0.055,  0.001,  0.017, -0.044, -0.058,  0.022, -0.078, -0.043,\n",
       "       -0.025,  0.237,  0.   , -0.004], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.get_vector(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The `KeyedVectors`-class provides many interesting methods on word-embeddings. For example the `most_similar(w)`-methode returns the words, whose word-vectors match best with the word-vector of `w`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cars', 0.73371422290802),\n",
       " ('vehicle', 0.7271659970283508),\n",
       " ('automobile', 0.7021709680557251),\n",
       " ('car--and', 0.7012600302696228),\n",
       " ('car.But', 0.6894583106040955),\n",
       " ('car.It', 0.6796760559082031),\n",
       " ('car.So', 0.679090142250061),\n",
       " ('car.Now', 0.6771497130393982),\n",
       " ('car.', 0.6755067706108093),\n",
       " ('car.When', 0.6720768809318542)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all words, whose vector is closer to the vector of *car* than the vector of *lorry*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vehicle',\n",
       " 'cars',\n",
       " 'Car',\n",
       " 'automobile',\n",
       " 'car.',\n",
       " 'car.The',\n",
       " 'car.It',\n",
       " 'car.But',\n",
       " 'car.So',\n",
       " 'car.When',\n",
       " 'car--',\n",
       " 'car.For',\n",
       " 'car.Now',\n",
       " 'car--and']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.closer_than(\"car\",\"truck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Glove Word-Embeddings\n",
    "After downloading word-embeddings from [Glove](https://nlp.stanford.edu/projects/glove/), they can be imported as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#GLOVE_DIR = \"./Data/glove.6B\"\n",
    "#GLOVE_DIR =\"/Users/maucher/DataSets/glove.6B\"\n",
    "GLOVE_DIR = '/Users/johannes/DataSets/Gensim/glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/68/9tltm6l520v0stj3qjlc5v9w0000gn/T/ipykernel_89913/1167505750.py:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec(glove_file, tmp_file)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "tmp_file = get_tmpfile(os.path.join(GLOVE_DIR, 'test_word2vec.txt'))\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('truck', 0.92085862159729),\n",
       " ('cars', 0.8870189785957336),\n",
       " ('vehicle', 0.8833683729171753),\n",
       " ('driver', 0.8464019298553467),\n",
       " ('driving', 0.8384189009666443),\n",
       " ('bus', 0.8210511803627014),\n",
       " ('vehicles', 0.8174992799758911),\n",
       " ('parked', 0.7902189493179321),\n",
       " ('motorcycle', 0.7866503000259399),\n",
       " ('taxi', 0.7833929657936096)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "toc_position": {
   "height": "643px",
   "left": "0px",
   "right": "1484px",
   "top": "125.233px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
